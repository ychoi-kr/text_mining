{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yong\\anaconda3\\envs\\textmining_cpu\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "image_size = 32  # We'll resize input images to this size\n",
    "patch_size = 4  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 2\n",
    "mlp_head_units = [512, 128]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size)\n",
    "    ],\n",
    "    name=\"preprocessing\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "preprocessing.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 0 Axes>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1780820ee80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(image.astype(\"uint8\"), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 32 X 32\n",
      "Patch size: 4 X 4\n",
      "Patches per image: 64\n",
      "Elements per patch: 16\n",
      "The shape of patches:  (1, 64, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD7CAYAAABOrvnfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJ70lEQVR4nO3dS0hU/xvH8ZMVZhllmbeyAiskKCrpQouwIKSSlBYWZEFtahVGYBRUixJCF4Ib2wQhgS0CyWXtNHBR2W0RpV3shpVJXooKtf8qyJnnkePvPzWjn/drN8985/vMsfn0hTNzvmfKr1+/fgUAJCTF+w0A+HcIPCCEwANCCDwghMADQgg8IITAA0IIPCCEwANCpoUdOGXKlL/5PoIgCALrR3/x6Kt0rPHqq3Ss8ewbiRUeEELgASEEHhBC4AEhBB4QQuABIQQeEELgASEEHhBC4AEhBB4QQuABIQQeEELgASEEHhBC4AEhBB4QQuABIaG3uAJiKSnJXmvKysrMemNjozvX7du3Q/f1toGaP3++We/t7Q0990TACg8IIfCAEAIPCCHwgBACDwiZ8ivM7vUAJgVWeEAIgQeEEHhACDeTNPoqHWu8+mZnZ5vjysvLzXplZaU7d3t7u1kvKioK+e6CIDMz06x//Pgx9By/Jcrf2MIKDwgh8IAQAg8IIfCAEAIPCOF6eIybdy17EARBVlZWqDm2b99u1ouLi8368PCwO1dHR4dZt87Se9fVf/361Z1/MmGFB4QQeEAIgQeEEHhACIEHhIS+Hl7pd95KxzpWX6+em5vrzr93716zXl1dPepxS0uLOW7NmjVm3RsfBEFw9OhRs/727duoWmpqqjk2lmfpE+nfNhIrPCCEwANCCDwghMADQgg8IITAA0K4eAZBcnKyWZ87d65ZP3bsmDvXiRMnQvXcsmWLWbe+SguCIHjw4IE717t370L1DAKdi2Q8rPCAEAIPCCHwgBACDwgh8IAQbiYJCGGFB4QQeEAIgQeEEHhACHePNfpOhmOtqKgw67W1tVG1ffv2mWMvXLhg1vPy8ty+3v7x06aN/qgNDQ2FGvfbw4cP3Z6lpaVm/eXLl+5rIsXyb58on2MLKzwghMADQgg8IITAA0IIPCCEfemNvhPlWNPT093nPn36FHqee/fumfWCggKzPjg46M518eJFsx55xv/UqVPmuP3797tze0pKSsz68+fPQ8/BWXoAkw6BB4QQeEAIgQeEEHhACNtUT2A9PT3uc1evXjXr5eXlUbXPnz+bY5uamsx6c3Oz27e1tdWsR56l985ae1tjd3R0uD27u7vd5yL9i7PliYwVHhBC4AEhBB4QQuABIQQeEELgASF8LTdJVVVVmXXra7nTp0+bYwcGBsz6s2fP3L6bN28O8e6CoLCw0KwnJdlr0P379925vn37FqonWOEBKQQeEELgASEEHhBC4AEh3D0WEMIKDwgh8IAQAg8I4WaSRl+lYx2rb1pamlkf69d01i/5gsC/YWUk7xd1J0+edF9z69Ytsz4R/sZ/u28kVnhACIEHhBB4QAiBB4QQeEAI18NPUtOmhf+nnTdvnlnfuXOnWT906JA7V35+fqienZ2dZv3KlStm3TsTj/FhhQeEEHhACIEHhBB4QAiBB4Rwlj6BpKammvXMzMxxjQ+CIJg9e3boviUlJWa9srLSrGdnZ7tz3blzx6zn5OSMelxTU2OOa2xsdOfG/48VHhBC4AEhBB4QQuABIQQeEELgASGht6lW2hpovD03bdrkPtfW1hZ6nj179pj1iooKs75+/Xp3rpSUlNB9R0ZGzLp3Y8enT5+6c507d86sX7t2bdRjpc9TPPtGYoUHhBB4QAiBB4QQeEAIgQeEcDNJQAgrPCCEwANCCDwghMADQrh7rNF3vD03btzoPldfX2/W165dG1V7//69OTY9Pd2sez99DQL/GKZOnRpV887benOM9dPaqqoqs97Q0DDq8Zw5c8xx/f397tzjlSifp3j2jcQKDwgh8IAQAg8IIfCAEAIPCGFf+hhYvny5+5x1Nt6TkZFh1r07wf78+dOd6/Xr12Z92bJlUbWzZ8+aY3Nzc836tm3b3L4HDx50n/tTc3OzWS8sLAz1evw3rPCAEAIPCCHwgBACDwgh8IAQtqk2+o6355IlS9zn6urqzPru3bujaq2trebYDx8+mPW7d++6fdvb2836zZs3o2qLFy82x3rfGtTW1rp9vTvdrlixwn3Nn2L5750on6d49o3ECg8IIfCAEAIPCCHwgBACDwgh8IAQLp6Jga6uLve56upqs259LedtD9XX12fWX7165fbt7u52n4v05s0bs56Tk2PWrW2yfgt7m4OhoaFQ4xBbrPCAEAIPCCHwgBACDwgh8IAQ7h4LCGGFB4QQeEAIgQeEcDNJo6/SsY7Vt6yszKyfOXPGnd/bOnvdunUh3h0bYMS6byRWeEAIgQeEEHhACIEHhBB4QEjCXw/vXXs9a9Yss+7deDEIgqC3tzcm72kiGOua9UgLFy40697W0jNnznTn6uzsNOuRZ+lv3LgR8t0hlljhASEEHhBC4AEhBB4QQuABIQl/lt47G7969Wqznp+f787V0tISqufKlSvNek9Pj1kfGBhw5xrP2fIZM2aY9eTkZLOelpbmzpWVlRW674EDB8ZVnz59ujtXW1ubWS8tLR31uKamJtybQ0yxwgNCCDwghMADQgg8IITAA0IIPCAk9DbV8dqix/vqyfvKqK6uzp3/8ePHZn3VqlUh3l0QXLp0yax7X0UFgX+hSX19fVRt69at5ti8vDyzHvlV15+Ki4vd58L68eOHWb98+bL7Guu4giD6b6+21RRbXAH45wg8IITAA0IIPCCEwANCuJkkIIQVHhBC4AEhBB4QQuABIRP27rFLly4164cPH3bnH+uup3/6/v27WR8ZGTHrfX197lzejjcZGRlRtcHBQXOst8PMWHvwj2enHc/58+fNekNDg/uaFy9emPXh4eFRjxPt8zRZ+0ZihQeEEHhACIEHhBB4QAiBB4Qk/AYYXl/vDHV2drY7/4YNG8z69evXRz0+fvy4OW7Xrl1mvaCgwO051t7xkb58+WLWu7q6zLq3T34Q+HdxPXLkSFStqKjIHPvo0aNx9x0aGjLrkf+2ifZ5mqx9I7HCA0IIPCCEwANCCDwghMADQibsWXpPUpL/f5h3J9r+/v5Rj70z/YsWLTLrCxYscHt6d35tamqKqu3YscMc6/3G3vvN/1ivefLkSVQtJSXFHOttU/1f9kzhLD1n6QH8YwQeEELgASEEHhBC4AEhBB4QMum+lotFX6VjjVdfpWONZ99IrPCAEAIPCCHwgBACDwgh8IAQ7h4LCGGFB4QQeEAIgQeETNibSf7NvkrHGq++Sscaz76RWOEBIQQeEELgASEEHhBC4AEhBB4QQuABIQQeEELgASEEHhBC4AEhBB4QQuABIQQeEELgASEEHhBC4AEhBB4QQuABIQQeEELgASEEHhBC4AEhBB4Qws0kASGs8IAQAg8IIfCAEAIPCCHwgBACDwgh8IAQAg8IIfCAkP8BKvzhCcBk0GQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x300 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "print(\"The shape of patches: \", patches.shape)\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(3, 3))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"), cmap='gray')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    The PatchEncoder layer will linearly transform a patch by projecting it into a vector of size projection_dim. In addition, it adds a learnable position embedding to the projected vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    preprocessed = preprocessing(inputs) # 입력 이미지에 대한 전처리 수행, 즉 표준화와 resize\n",
    "    patches = Patches(patch_size)(preprocessed) # 이미지를 패치 단위로 분할\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches) # 각 패치에 대한 임베딩 벡터 생성\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " preprocessing (Sequential)     (None, 32, 32, 1)    3           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " patches_1 (Patches)            (None, None, 16)     0           ['preprocessing[0][0]']          \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 64, 64)       5184        ['patches_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 64, 64)      128         ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 64, 64)      66368       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 64, 64)       0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 64, 64)      128         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64, 128)      8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64, 128)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64, 64)       8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 64)       0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 64)       0           ['dropout_1[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 64, 64)      128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 64, 64)      66368       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 64)       0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 64, 64)      128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64, 128)      8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64, 128)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64, 64)       8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64, 64)       0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 64)       0           ['dropout_3[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 64, 64)      128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 4096)         0           ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 512)          2097664     ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 512)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 10)           1290        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,336,333\n",
      "Trainable params: 2,336,330\n",
      "Non-trainable params: 3\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "checkpoint_filepath = \"./checkpoints/checkpoint_tr1\"\n",
    "mc = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', mode='min', \n",
    "                     save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 114s 264ms/step - loss: 1.4464 - accuracy: 0.5276 - val_loss: 0.3439 - val_accuracy: 0.9102\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 105s 249ms/step - loss: 0.6459 - accuracy: 0.7953 - val_loss: 0.2140 - val_accuracy: 0.9395\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 103s 245ms/step - loss: 0.4673 - accuracy: 0.8606 - val_loss: 0.1568 - val_accuracy: 0.9568\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 99s 235ms/step - loss: 0.3790 - accuracy: 0.8876 - val_loss: 0.1375 - val_accuracy: 0.9630\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 102s 241ms/step - loss: 0.3198 - accuracy: 0.9062 - val_loss: 0.1106 - val_accuracy: 0.9673\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 102s 243ms/step - loss: 0.2783 - accuracy: 0.9219 - val_loss: 0.0974 - val_accuracy: 0.9730\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 102s 241ms/step - loss: 0.2494 - accuracy: 0.9281 - val_loss: 0.0919 - val_accuracy: 0.9747\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 102s 241ms/step - loss: 0.2208 - accuracy: 0.9363 - val_loss: 0.0778 - val_accuracy: 0.9790\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 102s 243ms/step - loss: 0.1991 - accuracy: 0.9428 - val_loss: 0.0719 - val_accuracy: 0.9793\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 101s 241ms/step - loss: 0.1802 - accuracy: 0.9473 - val_loss: 0.0675 - val_accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train_one_hot, batch_size=128, epochs=10,\n",
    "                    validation_split=0.1, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1780bc34f70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 22ms/step - loss: 0.0754 - accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07541953772306442, 0.9768999814987183]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
